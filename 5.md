# Huge Db size in Ultimate POSâ€‹

**Goal**: Reduce database size by safely removing auto-generated duplicate entries in transaction_sell_lines_purchase_lines â€” one of the most bloated tables in UltimatePOS.

### âœ… What This Fix Did for Meâ€‹

- **Database Before**: ~4 GB
- **Database After Cleanup**: ~80 MB ğŸ˜²
- **âœ… No issues or data loss in sales or inventory after cleanup**

Run this SQL in `phpMyAdmin` (or any SQL tool):


```sql
SELECT COUNT(*) AS remaining_duplicates
FROM transaction_sell_lines_purchase_lines t
JOIN (
SELECT purchase_line_id, MIN(id) AS min_id
FROM transaction_sell_lines_purchase_lines
WHERE sell_line_id IS NULL
GROUP BY purchase_line_id
HAVING COUNT(*) > 1
) grouped ON t.purchase_line_id = grouped.purchase_line_id
WHERE t.sell_line_id IS NULL AND t.id != grouped.min_id;
```

### ğŸ” This shows how many duplicate rows exist where sell_line_id is NULL (a common duplication issue).

## ğŸ§¾ STEP 2: Preview 100 Duplicate Recordsâ€‹

**Preview 100 duplicates before deletion**:

```sql
SELECT dup.*
FROM transaction_sell_lines_purchase_lines AS dup
JOIN (
SELECT sell_line_id, purchase_line_id, MIN(id) AS min_id
FROM transaction_sell_lines_purchase_lines
GROUP BY sell_line_id, purchase_line_id
HAVING COUNT(*) > 1
) AS grouped
ON dup.sell_line_id = grouped.sell_line_id
AND dup.purchase_line_id = grouped.purchase_line_id
AND dup.id != grouped.min_id
LIMIT 100;
```

### ğŸ§  TIP: This won't delete anything â€” just lists the duplicates for your inspection.


## STEP 3: Safely Delete Duplicates in Batchesâ€‹

Start small! Try deleting 100 at a time:

```sql
DELETE FROM transaction_sell_lines_purchase_lines
WHERE id IN (
SELECT id FROM (
SELECT t.id
FROM transaction_sell_lines_purchase_lines t
JOIN (
SELECT sell_line_id, purchase_line_id, MIN(id) AS min_id
FROM transaction_sell_lines_purchase_lines
GROUP BY sell_line_id, purchase_line_id
HAVING COUNT(*) > 1
) grouped ON (t.sell_line_id <=> grouped.sell_line_id AND t.purchase_line_id = grouped.purchase_line_id)
WHERE t.id != grouped.min_id
LIMIT 100
) AS subquery
);
```

**ğŸ’¡ You can change LIMIT 100 to 500, 1000, or more based on**:

Server power (shared hosting, VPS, cloud)
Current load (run only during idle/off-peak time)


**Now perform optimize quiery** :

OPTIMIZE TABLE `transaction_sell_lines_purchase_lines`;


ğŸ“Œ Important Notesâ€‹

## âœ… What to DO:â€‹

- âœ”ï¸ Backup your database first!
- âœ”ï¸ Run during low or zero usage (midnight/off-hours)
- âœ”ï¸ Start with LIMIT 100 before increasing
- âœ”ï¸ Monitor your POS system after each delete run
- âœ”ï¸ Document total count before/after cleanup

## âŒ What NOT to DO:â€‹

- âŒ Don't run during business hours
- âŒ Donâ€™t delete without reviewing a sample first
- âŒ Donâ€™t blindly delete all in one go on shared servers
- âŒ Donâ€™t skip the backup â€” ever!


**You can connect me I will help :**
I suffered a lot and reach to solution even I can perform on live server No data losses will be happen

---
[â† Previous: Due Collections Feature](4.md)  |  [Next â†’: Delete Transaction, Product, Inventory](6.md)